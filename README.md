## ğŸ‡µğŸ‡± Wav2Vec2 Polish Speech Recognition Fine-tuning

## ğŸŒŸ State-of-the-Art Polish ASR

A comprehensive, performance-driven implementation of fine-tuning **Facebook's Wav2Vec2** model for Polish Automatic Speech Recognition (ASR). This project focuses on **advanced techniques for overfitting prevention** and achieving maximum accuracy from limited-resource training.

---

## ğŸš€ Quick Glance & Key Results

| Status       | Badge Placeholder                        |
| :----------- | :--------------------------------------- |
| **Model**    | `facebook/wav2vec2-large-xlsr-53-polish` |
| **Language** | Polish (PL)                              |
| **License**  | MIT                                      |

### The Improvement:

> **18% Relative WER Improvement** Achieved: **0.8477** (Base) â†’ **0.6954** (Fine-tuned)

---

## âœ¨ Key Features

- **Significant Performance Boost:** Achieved an **18% WER improvement** over the base XLSR-53 Polish model.
- **Overfitting Prevention:** Implements **advanced overfitting detection** and automatic **early stopping** based on validation metrics.
- **Memory Efficiency:** Optimizes training via a **frozen feature encoder** and gradient accumulation.
- **Comprehensive Pipeline:** Includes a full evaluation suite with baseline comparison.
- **Ease of Use:** **Ready-to-run Google Colab Notebooks** for immediate experimentation.

---

## ğŸ“Š Performance Comparison

This table highlights the reduction in Word Error Rate (WER) achieved by fine-tuning only the CTC head, demonstrating highly efficient utilization of trainable parameters.

| Model                               | Test WER (Lower is Better) | Relative Improvement | Trainable Parameters  |
| :---------------------------------- | :------------------------- | :------------------- | :-------------------- |
| **Base XLSR-53 Polish**             | 0.8477                     | -                    | 0 (Inference Only)    |
| **Fine-tuned Model (This Project)** | **0.6954**                 | **17.97%**           | \~25M (CTC Head Only) |

---

## ğŸ—ï¸ Technical Architecture

The fine-tuning pipeline is built upon proven MLOps best practices:

- **Base Model**: `facebook/wav2vec2-large-xlsr-53-polish`
- **Tokenization**: Character-level (42 Polish alphabet tokens + specials)
- **Training Strategy**: Frozen feature encoder + fine-tuning of the **CTC Prediction Head**
- **Loss Function**: Connectionist Temporal Classification (CTC) loss with padding mask.
- **Regularization**: Aggressive early stopping, weight decay, and gradient clipping.

---

## âš™ï¸ Quick Start

### Prerequisites

The following libraries are required.

```bash
# Required libraries
pip install \
    torch>=1.12.0 \
    torchaudio>=0.12.0 \
    transformers>=4.21.0 \
    datasets>=2.4.0 \
    librosa>=0.9.2 \
    jiwer>=2.5.0 \
    evaluate>=0.4.0
```

### ğŸ’» Google Colab Setup

1.  Open the **`wav2vec2_finetune_complete.ipynb`** notebook in Google Colab.
2.  Set the runtime to **GPU**: `Runtime` â†’ `Change runtime type` â†’ `Hardware accelerator` â†’ `GPU`.
3.  Execute all cells sequentially.

### ğŸ  Local Setup

```bash
git clone https://github.com/happy531/wav2vec2-polish-finetune.git
cd wav2vec2-polish-finetune
pip install -r requirements.txt
```

---

## ğŸ“ Repository Structure

```
wav2vec2-polish-finetune/
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ wav2vec2_finetune_complete.ipynb # ğŸš€ Main training notebook (start here)
â”‚ â””â”€â”€ wav2vec2_evaluation.ipynb      # Standalone final evaluation
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ data_preprocessing.py # Dataset loading and preparation
â”‚ â”œâ”€â”€ training_utils.py     # Training helpers and callbacks
â”‚ â””â”€â”€ evaluation.py         # Evaluation functions
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE (MIT)
```

---

## ğŸ”¬ Training Configuration & Overfitting Prevention

### Hyperparameters

The configuration is optimized for stable fine-tuning on consumer-grade GPUs:

| Parameter                   | Value | Justification                               |
| :-------------------------- | :---- | :------------------------------------------ |
| **Learning Rate**           | 1e-4  | Balanced rate for continued fine-tuning     |
| **Batch Size**              | 2     | GPU memory optimization for long sequences  |
| **Gradient Accumulation**   | 4     | Achieves an effective batch size of **8**   |
| **Max Steps**               | 1000  | Upper limit controlled by early stopping    |
| **Warmup Steps**            | 100   | Stabilizes initial gradients                |
| **Weight Decay**            | 0.01  | L2 regularization                           |
| **Early Stopping Patience** | 3     | Automatic halt on no validation improvement |

### Overfitting Monitoring

The critical role of **Early Stopping** is demonstrated below. Training is automatically stopped when the validation WER fails to improve after 3 epochs, preventing performance collapse.

| Training Step | Training Loss | Validation Loss | WER        | Note                        |
| :------------ | :------------ | :-------------- | :--------- | :-------------------------- |
| 100           | -340.56       | -383.63         | 0.7793     |                             |
| **200**       | -394.22       | -393.75         | **0.7034** | **Best WER achieved**       |
| 300           | -413.72       | -311.80         | 0.9172     | **âš ï¸ Overfitting starts**   |
| 400+          | -162.33       | -149.49         | 1.0000     | **âŒ Performance Collapse** |

---

## ğŸ’¾ Model Usage: Inference Example

Use the `Wav2Vec2ForCTC` class for easy loading and transcription.

```python
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
import torch
import librosa

# --- Load model and processor ---
MODEL_DIR = "./wav2vec2-polish-finetuned-v2"
model = Wav2Vec2ForCTC.from_pretrained(MODEL_DIR)
processor = Wav2Vec2Processor.from_pretrained(MODEL_DIR)

# --- Inference Example Function ---
def transcribe_audio(audio_path):
    # 1. Load and resample audio
    audio, sr = librosa.load(audio_path, sr=16000)

    # 2. Prepare inputs for model
    inputs = processor(audio, sampling_rate=16000, return_tensors="pt", padding=True)

    # 3. Predict logits
    with torch.no_grad():
        logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits

    # 4. Decode with argmax
    predicted_ids = torch.argmax(logits, dim=-1)
    transcription = processor.batch_decode(predicted_ids)

    return transcription

# Example usage:
# print(transcribe_audio("path/to/your/audio.wav"))
```

---

## ğŸ“š Technical Deep Dive

### Data Collation

- Utilizes the highly efficient `DataCollatorCTCWithPadding` for:
  - **Dynamic Padding**: Batches are padded to the maximum length of the sequences within that batch, reducing wasted memory.
  - **Loss Masking**: Padding tokens are automatically masked with `-100` to ensure they are **excluded from the CTC loss calculation**.

### Evaluation Metrics

- **Primary Metric**: **Word Error Rate (WER)** using the `jiwer` library.
- **Decoding**: Standard CTC decoding process: `argmax` over the output logits, followed by special token removal.
- **Validation**: Character-level comparison of the decoded transcript against the reference text.

---

## ğŸ“ˆ Advanced & Experimental Features

### Planned Extensions

- [ ] Audio data augmentation techniques (e.g., noise injection, time stretching).
- [ ] Integration with the **UASpeech** dataset for dysarthric speech recognition.
- [ ] Multi-dataset training and domain adaptation strategies.
- [ ] Severity-level performance analysis for impaired speech.

### Experimental Performance Summary

- **Memory Usage**: Remarkably low at **\~4-6 GB GPU VRAM**, significantly less than full Whisper fine-tuning (8-16 GB).
- **Training Speed**: Up to **5Ã— faster** than comparable end-to-end Whisper fine-tuning due to the frozen feature encoder.
- **Convergence**: Rapid, but requires the implemented **aggressive monitoring** due to high susceptibility to overfitting.

---

## ğŸ“„ Citation

If this work is useful for your research, please cite it:

```bibtex
@misc{wav2vec2-polish-finetune,
author = {JÄ™drzej WesoÅ‚owski},
title = {Wav2Vec2 Polish Speech Recognition Fine-tuning},
year = {2025},
publisher = {GitHub},
url = {https://github.com/happy531/wav2vec2-polish-finetune}
}
```

---

## ğŸ“ License & Contact

- **License**: This project is licensed under the **MIT License**. See the [LICENSE](https://www.google.com/search?q=LICENSE) file for details.
- **Contributing**: Pull Requests are welcome\! Focus areas include new augmentation or dataset integration.
- **Contact**: Feel free to reach out via [GitHub Issues](https://github.com/happy531/wav2vec2-polish-finetune/issues) for questions or collaboration.

---

### ğŸ™ Acknowledgments

- **Facebook AI Research** for the foundational Wav2Vec2 architecture.
- **Hugging Face** for the indispensable `transformers` library and model ecosystem.
- The Polish speech recognition community.

---

This implementation is part of a Master's thesis research on robust ASR for the Polish language under resource constraints.
