Perfect! Here's a comprehensive README.md for your GitHub repository:

---

```markdown
# Wav2Vec2 Polish Speech Recognition Fine-tuning

A comprehensive implementation of fine-tuning Facebook's Wav2Vec2 model for Polish automatic speech recognition (ASR) with advanced techniques for preventing overfitting and improving performance.

## ğŸ¯ Project Overview

This repository contains a complete pipeline for fine-tuning the `facebook/wav2vec2-large-xlsr-53-polish` model on custom Polish speech datasets. The implementation demonstrates significant performance improvements through careful hyperparameter tuning and overfitting prevention strategies.

### Key Features

- **18% WER improvement** over the base model (0.8477 â†’ 0.6954)
- **Advanced overfitting detection** and early stopping mechanisms
- **Memory-efficient training** with frozen feature encoder
- **Comprehensive evaluation pipeline** with baseline comparison
- **Ready-to-use Google Colab notebooks**

## ğŸ“Š Results

| Model               | Test WER | Relative Improvement | Trainable Parameters |
| ------------------- | -------- | -------------------- | -------------------- |
| Base XLSR-53 Polish | 0.8477   | -                    | 0 (inference only)   |
| Fine-tuned Model    | 0.6954   | **17.97%**           | ~25M (CTC head)      |

## ğŸ—ï¸ Architecture

The fine-tuning approach utilizes:

- **Pre-trained Model**: `facebook/wav2vec2-large-xlsr-53-polish`
- **Tokenization**: Character-level (42 Polish alphabet tokens + specials)
- **Training Strategy**: Frozen feature encoder + CTC head fine-tuning
- **Loss Function**: CTC loss with padding mask (-100)
- **Regularization**: Early stopping, weight decay, gradient clipping

## ğŸš€ Quick Start

### Prerequisites
```

# Required libraries

torch>=1.12.0
torchaudio>=0.12.0
transformers>=4.21.0
datasets>=2.4.0
librosa>=0.9.2
jiwer>=2.5.0
evaluate>=0.4.0

```

### Google Colab Setup
1. Open the notebook in Google Colab
2. Enable GPU runtime: Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU
3. Run all cells sequentially

### Local Setup
```

git clone https://github.com/happy531/wav2vec2-polish-finetune.git
cd wav2vec2-polish-finetune
pip install -r requirements.txt

```

## ğŸ“ Repository Structure

```

wav2vec2-polish-finetune/
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ wav2vec2_finetune_complete.ipynb # Main training notebook
â”‚ â””â”€â”€ wav2vec2_evaluation.ipynb # Standalone evaluation
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ data_preprocessing.py # Dataset preparation
â”‚ â”œâ”€â”€ training_utils.py # Training helpers
â”‚ â””â”€â”€ evaluation.py # Evaluation functions
â”œâ”€â”€ requirements.txt # Dependencies
â”œâ”€â”€ README.md # This file
â””â”€â”€ LICENSE # MIT License

```

## ğŸ”§ Training Configuration

| Parameter | Value | Justification |
|-----------|-------|---------------|
| Learning Rate | 1e-4 | Stable training for continued fine-tuning |
| Batch Size | 2 | GPU memory optimization for long sequences |
| Gradient Accumulation | 4 | Effective batch size: 8 |
| Max Steps | 1000 | Limited with early stopping |
| Warmup Steps | 100 | Gradient stabilization |
| Weight Decay | 0.01 | L2 regularization against overfitting |
| Early Stopping Patience | 3 | Automatic halt on no improvement |

## ğŸ“ˆ Training Process

### Step-by-Step Guide

1. **Environment Setup** - Install dependencies and configure GPU
2. **Dataset Loading** - Load and explore your Polish speech dataset
3. **Data Preprocessing** - Resample audio, tokenize transcripts
4. **Baseline Evaluation** - Test original model performance
5. **Training Configuration** - Set up trainer with early stopping
6. **Fine-tuning** - Train with overfitting monitoring
7. **Final Evaluation** - Compare results and save model

### Overfitting Prevention

The implementation includes robust overfitting detection based on validation WER monitoring:

| Training Step | Training Loss | Validation Loss | WER |
|---------------|---------------|-----------------|-----|
| 100 | -340.56 | -383.63 | 0.7793 |
| 200 | -394.22 | -393.75 | **0.7034** |
| 300 | -413.72 | -311.80 | 0.9172 âš ï¸ |
| 400+ | -162.33 | -149.49 | 1.0000 âŒ |

Early stopping prevents the dramatic performance degradation observed after step 200.

## ğŸ’¾ Model Usage

### Loading Fine-tuned Model
```

from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
import torch

# Load model and processor

model = Wav2Vec2ForCTC.from_pretrained("./wav2vec2-polish-finetuned-v2")
processor = Wav2Vec2Processor.from_pretrained("./wav2vec2-polish-finetuned-v2")

# Inference example

def transcribe_audio(audio_path):
audio, sr = librosa.load(audio_path, sr=16000)
inputs = processor(audio, sampling_rate=16000, return_tensors="pt", padding=True)

    with torch.no_grad():
        logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits

    predicted_ids = torch.argmax(logits, dim=-1)
    transcription = processor.batch_decode(predicted_ids)

    return transcription

```

## ğŸ“š Technical Details

### Data Collation
Uses `DataCollatorCTCWithPadding` for:
- Dynamic sequence padding to batch maximum length
- Label masking with -100 to exclude padding from loss calculation
- Efficient memory utilization with variable-length sequences

### Evaluation Metrics
- **Primary**: Word Error Rate (WER) using `jiwer` library
- **Implementation**: CTC decoding with argmax + special token removal
- **Validation**: Character-level comparison of predicted vs reference sequences

## ğŸ”¬ Advanced Features

### Planned Extensions
- [ ] Data augmentation with audio transformations
- [ ] UASpeech dataset integration for dysarthric speech
- [ ] Multi-dataset training strategies
- [ ] Severity-level performance analysis

### Experimental Results
- **Memory Usage**: ~4-6 GB GPU memory (vs 8-16 GB for full Whisper)
- **Training Speed**: 5Ã— faster than comparable Whisper fine-tuning
- **Convergence**: Requires careful monitoring due to overfitting susceptibility

## ğŸ“„ Citation

If you use this work in your research, please cite:

```

@misc{wav2vec2-polish-finetune,
author = {JÄ™drzej WesoÅ‚owski},
title = {Wav2Vec2 Polish Speech Recognition Fine-tuning},
year = {2025},
publisher = {GitHub},
url = {https://github.com/happy531/wav2vec2-polish-finetune}
}

```

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request. Areas for improvement:
- Additional data augmentation techniques
- Support for other Polish speech datasets
- Performance optimization strategies
- Multi-GPU training support

## ğŸ“ Contact

For questions or collaboration opportunities, please reach out via:
- GitHub Issues: [Create an issue](https://github.com/happy531/wav2vec2-polish-finetune/issues)

## ğŸ™ Acknowledgments

- Facebook AI Research for the Wav2Vec2 architecture
- Hugging Face for the transformers library and model hosting
- The Polish speech recognition community
- Contributors to the XLSR-53 multilingual model

---

**Note**: This implementation is part of a Master's thesis research on automatic speech recognition for Polish language with limited training resources.
```
